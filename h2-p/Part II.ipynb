{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Non-Terminal Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "df = pd.read_csv('cfg.counts', sep=' ', names=['Count', 'Rule Type', 'Rule'])\n",
    "\n",
    "nonterminal_count = defaultdict(int)\n",
    "for count, _, nonterminal in df[df['Rule Type'] == 'NONTERMINAL'].values:\n",
    "    nonterminal_count[nonterminal] = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $q_\\text{MLE}(X \\rightarrow Y_1Y_2 | X) = \\frac{\\text{count}(X \\rightarrow Y_1Y_2)}{\\text{count}(X)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('cfg.counts', sep=' ', names=['Count', 'Rule Type', 'LHS', 'One', 'Two'])\n",
    "\n",
    "binary_rules = df[df['Rule Type'] == 'BINARYRULE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/v/filer4b/v20q001/ebanner/.virtualenvs/yupa-2.7/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from IPython.kernel.zmq import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>LHS</th>\n",
       "      <th>One</th>\n",
       "      <th>Two</th>\n",
       "      <th>MLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>1</td>\n",
       "      <td>VP</td>\n",
       "      <td>VERB</td>\n",
       "      <td>SBAR+VP</td>\n",
       "      <td>0.001245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>4</td>\n",
       "      <td>VP</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NP+PRON</td>\n",
       "      <td>0.004981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>1</td>\n",
       "      <td>SBAR+S+VP</td>\n",
       "      <td>PRT</td>\n",
       "      <td>VP+VERB</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>63</td>\n",
       "      <td>SQ</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VP</td>\n",
       "      <td>0.030274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>1</td>\n",
       "      <td>SQ</td>\n",
       "      <td>ADVP+ADV</td>\n",
       "      <td>VP+VERB</td>\n",
       "      <td>0.000481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Count        LHS       One      Two       MLE\n",
       "519      1         VP      VERB  SBAR+VP  0.001245\n",
       "520      4         VP      VERB  NP+PRON  0.004981\n",
       "521      1  SBAR+S+VP       PRT  VP+VERB  0.100000\n",
       "522     63         SQ      VERB       VP  0.030274\n",
       "523      1         SQ  ADVP+ADV  VP+VERB  0.000481"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = lambda row: row['Count'] / float(nonterminal_count[row['LHS']])\n",
    "binary_rules['MLE'] = binary_rules.apply(fn, axis=1)\n",
    "del binary_rules['Rule Type']\n",
    "\n",
    "binary_rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "mle = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for count, lhs, one, two, maximum_likelihood_estimate in binary_rules.values:\n",
    "    mle[(lhs, (one, two))] = maximum_likelihood_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert mle[('VP', ('VERB', 'SBAR+VP'))] == binary_rules.MLE.values[0]\n",
    "assert mle[('VP', ('VERB', 'NP+PRON'))] == binary_rules.MLE.values[1]\n",
    "assert mle[('SBAR+S+VP', ('PRT', 'VP+VERB'))] == binary_rules.MLE.values[2]\n",
    "assert mle[('SQ', ('VERB', 'VP'))] == binary_rules.MLE.values[3]\n",
    "assert mle[('SQ', ('ADVP+ADV', 'VP+VERB'))] == binary_rules.MLE.values[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $q_\\text{MLE}(X \\rightarrow w) = \\frac{\\text{count}(X \\rightarrow w)}{\\text{count}(X)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('cfg.counts', sep=' ', names=['Count', 'Rule Type', 'Constituent', 'Token'])\n",
    "\n",
    "unary_rules = df[df['Rule Type'] == 'UNARYRULE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/v/filer4b/v20q001/ebanner/.virtualenvs/yupa-2.7/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from IPython.kernel.zmq import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Constituent</th>\n",
       "      <th>Token</th>\n",
       "      <th>MLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>7</td>\n",
       "      <td>.</td>\n",
       "      <td>_RARE_</td>\n",
       "      <td>0.003445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1</td>\n",
       "      <td>PRT</td>\n",
       "      <td>_RARE_</td>\n",
       "      <td>0.003571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>653</td>\n",
       "      <td>WHNP+PRON</td>\n",
       "      <td>What</td>\n",
       "      <td>0.799266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>3</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>British</td>\n",
       "      <td>0.000779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>25</td>\n",
       "      <td>ADP</td>\n",
       "      <td>as</td>\n",
       "      <td>0.024752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Count Constituent    Token       MLE\n",
       "62      7           .   _RARE_  0.003445\n",
       "63      1         PRT   _RARE_  0.003571\n",
       "64    653   WHNP+PRON     What  0.799266\n",
       "65      3        NOUN  British  0.000779\n",
       "66     25         ADP       as  0.024752"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = lambda row: row['Count'] / float(nonterminal_count[row['Constituent']])\n",
    "unary_rules['MLE'] = unary_rules.apply(fn, axis=1)\n",
    "del unary_rules['Rule Type']\n",
    "unary_rules.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache Unary Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for count, constituent, token, maximum_likelihood_estimate in unary_rules.values:\n",
    "    mle[(constituent, token)] = maximum_likelihood_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert mle[('.', '_RARE_')] == unary_rules.MLE.values[0]\n",
    "assert mle[('PRT', '_RARE_')] == unary_rules.MLE.values[1]\n",
    "assert mle[('WHNP+PRON', 'What')] == unary_rules.MLE.values[2]\n",
    "assert mle[('NOUN', 'British')] == unary_rules.MLE.values[3]\n",
    "assert mle[('ADP', 'as')] == unary_rules.MLE.values[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Rare Words and All Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('cfg.counts', sep=' ', names=['Count', 'Rule Type', 'Rule', 'Token'])\n",
    "\n",
    "unaries = df[df['Rule Type'] == 'UNARYRULE']\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "counts = defaultdict(int)\n",
    "\n",
    "for count, token in unaries[['Count', 'Token']].values:\n",
    "    counts[token] += int(count)\n",
    "    \n",
    "rares = set(token for token, count in counts.items() if count < 5)\n",
    "all_words = set(ant for noterminal, ant in mle if not isinstance(ant, tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All Nonterminals and Binary Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonterminals = nonterminal_count.keys()\n",
    "binaries = [(lhs, (one, two)) for count, lhs, one, two, maximum_likelihood_estimate in binary_rules.values ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKY Sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base case initialization...\n",
      "\n",
      "pi(i=0, i=0, X=VP+VERB) = mle(X=VP+VERB -> w=called) = 0.0708661417323\n",
      "pi(i=0, i=0, X=VERB) = mle(X=VERB -> w=called) = 0.00595998297148\n",
      "pi(i=0, i=0, X=NP+VERB) = mle(X=NP+VERB -> w=called) = 0.5\n",
      "pi(i=1, i=1, X=.) = mle(X=. -> w=?) = 0.834153543307\n",
      "\n",
      "\n",
      "Recursive steps...\n",
      "\n",
      "gap = 1\n",
      "    i = 0\n",
      "        j = 1\n",
      "            split point = 0\n",
      "\n",
      "                Computing pi(\"called | ?\", VP -> VERB .)...\n",
      "                ============================================================\n",
      "                pi(i=0, j=1, X=VP) = mle(VP -> VERB .) *\n",
      "                                   pi(i=0, s=0, Y1=VERB) *\n",
      "                                   pi(s+1=1, j=1, Y2=.)...\n",
      "\n",
      "                mle(VP -> VERB .) = 0.00124533001245\n",
      "                pi(i=0, s=0, Y_1=VERB) = 0.00595998297148\n",
      "                pi(s+1=1, j=1, Y_2=.) = 0.834153543307\n",
      "\n",
      "                Total = 6.19120910798e-06\n",
      "\n",
      "                Previously pi(i=0, j=1, X=VP) = 0.0\n",
      "                Previously bp(i=0, j=1, X=VP) = None\n",
      "\n",
      "                Now        pi(i=0, j=1, X=VP) = 6.19120910798e-06\n",
      "                Now        bp(i=0, j=1, X=VP) = (('VERB', '.'), 0)\n",
      "                ============================================================\n",
      "\n",
      "\n",
      "                Computing pi(\"called | ?\", NP -> VERB .)...\n",
      "                ============================================================\n",
      "                pi(i=0, j=1, X=NP) = mle(NP -> VERB .) *\n",
      "                                   pi(i=0, s=0, Y1=VERB) *\n",
      "                                   pi(s+1=1, j=1, Y2=.)...\n",
      "\n",
      "                mle(NP -> VERB .) = 0.000415541242468\n",
      "                pi(i=0, s=0, Y_1=VERB) = 0.00595998297148\n",
      "                pi(s+1=1, j=1, Y_2=.) = 0.834153543307\n",
      "\n",
      "                Total = 2.06588028826e-06\n",
      "\n",
      "                Previously pi(i=0, j=1, X=NP) = 0.0\n",
      "                Previously bp(i=0, j=1, X=NP) = None\n",
      "\n",
      "                Now        pi(i=0, j=1, X=NP) = 2.06588028826e-06\n",
      "                Now        bp(i=0, j=1, X=NP) = (('VERB', '.'), 0)\n",
      "                ============================================================\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-309d9202c778>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconstituent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_point\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_point\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0mreconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SBARQ'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-309d9202c778>\u001b[0m in \u001b[0;36mreconstruct\u001b[1;34m(i, j, constituent)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_point\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackpointer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstituent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconstituent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_point\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_point\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "sentence = 'called ?'\n",
    "\n",
    "# Split the sentence and map rare words to _RARE_\n",
    "sentence = ['_RARE_' if token not in all_words or token in rares else token for token in sentence.split()]\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "highest_prob, backpointer = defaultdict(float), defaultdict(lambda: None)\n",
    "\n",
    "# Base case\n",
    "print 'Base case initialization...'\n",
    "print\n",
    "for i, token in enumerate(sentence):\n",
    "    for constituent in nonterminals:\n",
    "        if mle[(constituent, token)]:\n",
    "            print 'pi(i={}, i={}, X={}) = mle(X={} -> w={}) = {}'.format(i, i, constituent, constituent, token, mle[(constituent, token)])\n",
    "        highest_prob[(i, i, constituent)] = mle[(constituent, token)]\n",
    "\n",
    "print\n",
    "print\n",
    "print 'Recursive steps...'\n",
    "print\n",
    "for gap in range(1, len(sentence)):\n",
    "    print 'gap = {}'.format(gap)\n",
    "    for i, token in enumerate(sentence[:-gap]):\n",
    "        print '    i = {}'.format(i)\n",
    "        j = i + gap\n",
    "        print '        j = {}'.format(j)\n",
    "        for split_point in range(i, j):\n",
    "            print '            split point = {}'.format(split_point)\n",
    "            for X, (Y1, Y2) in binaries:\n",
    "                m = mle[(X, (Y1, Y2))] * highest_prob[(i, split_point, Y1)] * highest_prob[(split_point+1, j, Y2)]\n",
    "\n",
    "                # Non-zero probability?\n",
    "                if m:          \n",
    "                    print\n",
    "                    print '                Computing pi(\"{} | {}\", {} -> {} {})...'.format(' '.join(sentence[i:split_point+1]), ' '.join(sentence[split_point+1:j+1]), X, Y1, Y2)\n",
    "                    print '                ============================================================'\n",
    "                    print '                pi(i={}, j={}, X={}) = mle({} -> {} {}) *'.format(i, j, X, X, Y1, Y2)\n",
    "                    print '                                   pi(i={}, s={}, Y1={}) *'.format(i, split_point, Y1)\n",
    "                    print '                                   pi(s+1={}, j={}, Y2={})...'.format(split_point+1, j ,Y2)\n",
    "                    print\n",
    "                    print '                mle({} -> {} {}) = {}'.format(X, Y1, Y2, mle[(X, (Y1, Y2))])\n",
    "                    print '                pi(i={}, s={}, Y_1={}) = {}'.format(i, split_point, Y1, highest_prob[(i, split_point, Y1)])\n",
    "                    print '                pi(s+1={}, j={}, Y_2={}) = {}'.format(split_point+1, j, Y2, highest_prob[(split_point+1, j, Y2)])\n",
    "                    print\n",
    "                    print '                Total = {}'.format(m)\n",
    "                    print\n",
    "                    print '                Previously pi(i={}, j={}, X={}) = {}'.format(i, j, X, highest_prob[(i, j, X)])\n",
    "                    print '                Previously bp(i={}, j={}, X={}) = {}'.format(i, j, X, backpointer[(i, j, X)])\n",
    "                    print\n",
    "                    \n",
    "                    if m > highest_prob[(i, j, X)]:\n",
    "                        highest_prob[(i, j, X)] = m\n",
    "                        backpointer[(i, j, X)] = ((Y1, Y2), split_point)\n",
    "                    \n",
    "                    print '                Now        pi(i={}, j={}, X={}) = {}'.format(i, j, X, highest_prob[(i, j, X)])\n",
    "                    print '                Now        bp(i={}, j={}, X={}) = {}'.format(i, j, X, backpointer[(i, j, X)])\n",
    "                    print '                ============================================================'\n",
    "                    print\n",
    "\n",
    "\n",
    "def reconstruct(i, j, constituent):\n",
    "    \"\"\"\"Follow backpointers to reconstruct parse tree\"\"\"\n",
    "    if i == j:\n",
    "        return [constituent, sentence[i]]\n",
    "    \n",
    "    else:\n",
    "        (y, z), split_point = backpointer[(i, j, constituent)]\n",
    "        return [constituent, reconstruct(i, split_point, y), reconstruct(split_point+1, j, z)]\n",
    "\n",
    "reconstruct(0, len(sentence)-1, 'SBARQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cky(sentence, mle, rares, all_words):\n",
    "    \"\"\"\"CKY algorithm for parsing a sentence\"\"\"\n",
    "    \n",
    "    # Split the sentence and map rare words to _RARE_\n",
    "    sentence = ['_RARE_' if token not in all_words or token in rares else token for token in sentence.split()]\n",
    "    \n",
    "    from collections import defaultdict\n",
    "    highest_prob, backpointer = defaultdict(float), defaultdict(lambda: None)\n",
    "\n",
    "    # Base case\n",
    "    for i, token in enumerate(sentence):\n",
    "        for constituent in nonterminals:\n",
    "            highest_prob[(i, i, constituent)] = mle[(constituent, token)]\n",
    "    \n",
    "    # Recursion\n",
    "    for gap in range(1, len(sentence)):\n",
    "        for i, token in enumerate(sentence[:-gap]):\n",
    "            j = i + gap\n",
    "            for split_point in range(i, j):\n",
    "                for X, (Y1, Y2) in binaries:\n",
    "                    m = mle[(X, (Y1, Y2))] * highest_prob[(i, split_point, Y1)] * highest_prob[(split_point+1, j, Y2)]\n",
    "                    # Non-zero probability?\n",
    "                    if m:          \n",
    "                        if m > highest_prob[(i, j, X)]:\n",
    "                            highest_prob[(i, j, X)] = m\n",
    "                            backpointer[(i, j, X)] = ((Y1, Y2), split_point)\n",
    "\n",
    "                        \n",
    "    def reconstruct(i, j, constituent):\n",
    "        \"\"\"\"Follow backpointers to reconstruct parse tree\"\"\"\n",
    "        if i == j:\n",
    "            return [constituent, sentence[i]]\n",
    "\n",
    "        else:\n",
    "            (y, z), split_point = backpointer[(i, j, constituent)]\n",
    "            return [constituent, reconstruct(i, split_point, y), reconstruct(split_point+1, j, z)]\n",
    "        \n",
    "\n",
    "    return reconstruct(0, len(sentence)-1, 'SBARQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Development Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('parse_dev.dat', 'r') as f:\n",
    "    questions = [line.strip() for line in f.readlines()]\n",
    "\n",
    "parses = [cky(question, mle, rares, all_words) for question in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('parse_dev.out', 'w') as f:\n",
    "    for parse in parses:\n",
    "        json.dump(parse, f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python eval_parser.py parse_dev.key parse_dev.out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
